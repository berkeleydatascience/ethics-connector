<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Literature-connector by dsten</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">INFO 88 Data and Ethics</h1>
      <h2 class="project-tagline">A Spring 2016 Data Science Connector Course </h2>
      <h3>CCN: 41505 | Anna Lauren Hoffman | Tuesday 3:30-5:30 PM | 210 South Hall | Units: 2</h3>
      <a href="http://data8.org/" class="btn">Data8.org</a>
      <a href="http://databears.berkeley.edu/" class="btn">Databears</a>
      <a href="http://www.ischool.berkeley.edu/courses/i88a" class="btn">Course Page</a>
    </section>

    <section class="main-content">
      <h3>Welcome to INFO 88 Data and Ethics</h3>
 

<p>This course provides an introduction to critical and ethical issues surrounding data and society. It blends social and historical perspectives on data with ethics, policy, and case examples—from Facebook’s “Emotional Contagion” experiment to search engine algorithms to self-driving cars—to help students develop a workable understanding of current ethical issues in data science. Ethical and policy-related concepts addressed include: research ethics; privacy and surveillance; data and discrimination; and the “black box” of algorithms. Importantly, these issues will be addressed throughout the lifecycle of data—from collection to storage to analysis and application.</p>

<h3>Course Objectives</h3>
<p>Upon completion of the course, students will 1) identify and articulate some basic ethical and policy-based frameworks; 2) understand the relationship between data, ethics, and society; and 3) be able to critically assess their own work and education in the area of data science. In particular, course assignments will emphasize researcher and practitioner reflexivity, allowing students to explore their own social and ethical commitments as future data scientists and information professionals.</p>


  </body>
  
<p>For more information on the Data Science Education Program at UC Berkeley, please visit databears.berkeley.edu</p>

<h3>Contact</h3>
<p>Instructor: Anna Laura Hoffman</p>
<p>Email: annalauren@berkeley.edu</p>
<p>Office Hours: Tuesdays 2:00-3:30 PM</p>
<p>Office Hours Location: South Hall 302</p>

<h3>Course Schedule</h3>
<b>Module 1- Situating "Data" I: What is data?</b>
<p>Objective: to "shake loose" the idea of data as an object for critical and ethical inquiry</p>
<u>Reading(s):</u>
<ul>
<li>Kitchin,  R.  (2014). Conceptualising Data. In  The data  revolution  (pp.  1-26).  New York: SAGE.</li>
</ul>

<b>Module 2- Situating "Data" II: A pre-history of data</b>
<p>Objective: to explore some historical precedents of today's "big data" moment</p>
<p>Case(s): Censuses & Area Codes</p>
<u>Reading(s):</u>
<ul>
<li>Seltzer,  W., & Anderson, M.  (2001). The dark  side  of  numbers:  The role  of  population  data  
systems in  human rights  abuses. Social  Research, 68(2),  481-513.</li>
<li>Garber, M.  (2014,  February  13).  Our numbered  days: The evolution of  the area  code. The
Atlantic. Retrieved from  http://www.theatlantic.com/technology/archive/2014/02/our-
numbered-days-the-evolution-of-the-area-code/283803/</li>
</ul>

<b>Module 3- Ethical Toolbox I: Research and applied ethics</b>
<p>Objective: introduce and explore applied ethical frameworks for thinking about data</p>
<p>Case(s): Facebook’s emotional contagion experiment; OK Cupid match rank testing</p>
<u>Reading(s):</u>
<ul>
<li>The Belmont Report. (1979). The Belmont Report: Ethical principles  and guidelines  for the 
protection  of  human subjects  of  research. Retrieved from  
http://www.hhs.gov/ohrp/humansubjects/guidance/belmont.html</li>
<li>Gray, M.  (2014,  July  9). When  science,  customer  service,  and human subjects  research  collide.  
Now what? Culture Digitally.  Retrieved from  http://culturedigitally.org/2014/07/when-science-
customer-service-and-human-subjects-research-collide-now-what/</li>
</ul>
<p>Short Writing Assignment #1 due before start of class</p>

<b>Module 4- Ethical Toolbox II: Concepts of privacy and publicity</b>
<p>Objective: explore basic concepts  of  privacy and anonymity (access, control, and context)</p>
<p>Case(s): 2006 AOL search data release; Facebook “Taste, Ties, Time” data release</p>
<u>Reading(s):</u>
<ul>
<li>Tavani, H.  (2012). Privacy and cyberspace. In  Ethics  and technology: Controversies,  questions,  
and strategies  for ethical computing,  4th Edition (pp.  131-168). Hoboken,  NJ: Wiley.</li>
<li>Barocas,  S., & Nissenbaum, H.  (2014,  November).  Big data’s  end run around  procedural  privacy 
protections:  Recognizing the inherent  limitations of  consent and anonymity.  Communications  of  
the ACM,  57(11), 31-33.</li>
</ul>

<b>Module 5- Lifecycle of Data I (Part I): Issues in data collection and data mining</b>
<p>Objective: attend to ethical questions in the collection and mining  of online  data</p>
<p>Case(s): data mining, social games, consent, Terms of  Service</p>
<u>Reading(s):</u>
<ul>
<li>van Wel,  L., & Royakkers,  L.  (2004). Ethical issues  in  data  mining. Ethics  and Information
Technology, 6,  129-140.</li>
<li>Willson,  M., & Leaver, T.  (2015). Zynga’s FarmVille,  social  games,  and the ethics  of  big data  
mining. Communication and Research  Practice, 1(2), 147-158.</li>
</ul>

<b>Module 6- Lifecycle of Data I (Part II): Issues in data collection and data mining</b>
<p>Objective: (cont’d from Module 5)</p>
<p>Case(s): data collection, personal fitness trackers, and the Quantified Self</p>
<u>Reading(s):</u>
<ul>
<li>Crawford, K., Lingel, J., & Karppi, T. (2015).  Our metrics,  ourselves:  A hundred years of  self-
tracking  from  the weight  scale to  the wrist wearable  device. European  Journal of  Cultural  
Studies,  18(4-5),  479-496.</li>
<li>Eveleth,  R.  (2014,  December  15).  How self-tracking apps  exclude women.  The Atlantic.
Retrieved from  http://www.theatlantic.com/technology/archive/2014/12/how-self-tracking-
apps-exclude-women/383673/</li>
<li>Watson, S.M.  (2014,  September 25).  Stepping  down: Rethinking  the fitness tracker.  The 
Atlantic. Retrieved from  http://www.theatlantic.com/technology/archive/2014/12/how-self-
tracking-apps-exclude-women/383673/</li>
</ul>
<p>Short Writing Assignment #2 due before the start of class</p>

<b>Module 7- Lifecycle of Data II: Issues in data storage and security</b>
<p>Objective: explore ethical and privacy issues  in  data, information, and computer security</p>
<p>Case(s): educational data and students’ privacy</p>
<u>Reading(s):</u>
<ul>
<li>Brey, P.  (2007). Ethical aspects of  information security  and privacy.  In  M.  Petković  & W.  Jonker  
(eds.), Security, privacy,  and trust in  modern  data  management (pp. 21-36). New York: Springer.</li>
<li>boyd, d.  (2015,  May 22).  Which students  get to  have  privacy?  The Message [Web  log post].  
Retrieved from  https://medium.com/message/which-students-get-to-have-privacy-
e9773f9a064#.urtohca12</li>
</ul>

<b>Module 8- Lifecycle of Data III (Part I): Issues in analyzing and exploring data</b>
<p>Objectives: building on discussions from Module 5, tackling ethical issues in data analysis</p>
<p>Case(s): “Spurious Correlations,” app design, data inclusion</p>
<u>Reading(s):</u>
<ul>
<li>boyd, d., & Crawford, K.  (2012). Critical  questions for big data. Information,  Communication,  
and Society,  15(5),  662-679.  (Introduction and Section 1 “Big  Data  changes the definition  of  
knowledge”)</li>
<li>Carr, N.  (2014,  April 16).  The limits  of  social  engineering.  MIT Technology  Review. Retrieved 
from  http://www.technologyreview.com/review/526561/the-limits-of-social-engineering/</li>
<li>Ananny, M.  (2011,  April 14).  The curious connection  between apps  for gay men and sex 
offenders.  The Atlantic. Retrieved from  
http://www.theatlantic.com/technology/archive/2011/04/the-curious-connection-between-
apps-for-gay-men-and-sex-offenders/237340/</li>
<li>“Spurious Correlations” http://www.tylervigen.com/spurious-correlations</li>
</ul>

<b>Module 9- Lifecycle of Data III (Part II): Issues in analyzing and exploring data</b>
<p>Objectives: (cont’d from Module 8)</p>
<p>Case(s): Hurricane Sandy, marginalized populations, data exclusions</p>
<u>Reading(s):</u>
<ul>
<li>Lerman, J.  (2013,  September 3). Big data  and its exclusions. Stanford  Law Review  Online, 66, 55-
63.</li>
<li>Crawford, K.  (2013,  April 1). The hidden  biases  in  big data. Harvard Business  Review. Retrieved 
from  https://hbr.org/2013/04/the-hidden-biases-in-big-data/</li>
<li>Chalabi,  M.  (2014,  July  29).  Why we  don’t know  the size  of  the transgender population. 
FiveThirtyEight.  Retrieved from  http://fivethirtyeight.com/features/why-we-dont-know-the-
size-of-the-transgender-population/</li>
</ul>
<p>Final Essay Proposals due before the start of class</p>

<p><b>SPRING BREAK</b></p>

<b>Module 10- Lifecycle of Data IV (Part I): Ethics of algorithms and automated systems</b>
<p>Objectives: building on Modules 8/9, examining consequences of automation and 
implementation</p>
<p>Case(s): algorithmic cruelty, data and discrimination</p>
<u>Reading(s):</u>
<ul>
<li>Gillespie,  T.  (2012). Can an  algorithm be  wrong?  Limn, 2,  n.p.  Retrieved from  
http://escholarship.org/uc/item/0jk9k4hj</li>
<li>Slavin, K.  (2011). How algorithms  shape our world [video].  TEDGlobal 2011. Retrieved from
http://www.ted.com/talks/kevin_slavin_how_algorithms_shape_our_world</li>
<li>Wolf, C., & Polonetsky, Jules.  (2014,  November  5). Big data: Putting the heat  on  hate. Re/code.  
Retrieved from  http://recode.net/2014/11/05/big-data-putting-heat-on-the-hate/</li>
<li>Meyer,  E. (2014, December  24).  Inadvertent algorithmic cruelty.  MeyerWeb.com. Retrieved 
from  http://meyerweb.com/eric/thoughts/2014/12/24/inadvertent-algorithmic-cruelty/</li>
</ul>

<b>Module 11- Lifecycle of Data IV (Part II): Ethics of algorithms and automated systems</b>
<p>Objectives: (cont’d from  Module 10)</p>
<p>Case(s): Google search, redlining, race, and gender</p>
<u>Reading(s):</u>
<ul>
<li>Noble,  S.U.  (2012,  Spring).  Missed  connections:  What  search  engines say about women.  Bitch 
Magazine, 54, 37-41.  Retrieved from  
https://safiyaunoble.files.wordpress.com/2012/03/54_search_engines.pdf</li>
<li>Barr, A.  (2015,  July  1). Google  mistakenly  tags  black people  as  ‘gorillas,’ showing limits  of  
algorithms. WSJ Bits  Blog. Retrieved from  http://blogs.wsj.com/digits/2015/07/01/google-
mistakenly-tags-black-people-as-gorillas-showing-limits-of-algorithms/</li>
<li>Mock, B.  (2015,  September 28).  Redlining is  alive and well—and  evolving. CityLab.  Retrieved 
from  http://www.citylab.com/housing/2015/09/redlining-is-alive-and-welland-
evolving/407497/</li>
</ul>

<b>Module 12- Lifecycle of Data V: Issues in dissemination and evaluation of data</b>
<p>Objectives: Trace ethical challenges in the evaluation and communication of results</p>
<p>Case(s): Google Flu trends, United  Nation  Waste Crimes report</p>
<u>Reading(s):</u>
<ul>
<li>Harris, J.  (2014,  May 22).  Distrust  your  data. Source.opennews.org.  Retrieved from  
https://source.opennews.org/en-US/learning/distrust-your-data/</li>
<li>Madrigal, A.C.  (2015,  October 6). The deception that  lurks in  our data-driven world.  Fusion. 
Retrieved from  http://fusion.net/story/202230/true-data-can-lie/</li>
<li>Lepawsky, J., Goldstein,  J., & Schulz, Y.  (2015,  June  24).  Criminal  negligence? Discard studies:  
Social  studies of  waste,  pollution,  & externalities.  Retrieved from  
http://discardstudies.com/2015/06/24/criminal-negligence/</li>
<li>Lazer,  D., & Kennedy,  R.  (2015,  October 1). What  we  can learn from  the epic  failure of  Google  
flu trends. Wired.  Retrieved from  http://www.wired.com/2015/10/can-learn-epic-failure-
google-flu-trends/</li>
</ul>

<b>Module 13- Interrogating Data Science: Asking critical and ethical questions</b>
<p>Objectives: strategies for thinking pragmatically about ethics as an info professional</p>
<p>Student-generated session – readings to be chosen and lecture to be led by student groups, facilitated by instructor</p>

<b>Module 14- Data Futures: Thinking ethically, thinking ahead</b>
<p>Objectives: identify and explore ethical issues on data science’s horizons</p>
<p>Student-generated session – readings to be chosen and lecture to be led by student groups, 
facilitated by instructor</p>

<b>Module 15- Course Reflection: Revisiting our Data Doubles</b>
<p>Final Essay Projects due</p>
  
</html>
